{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[NorMistral-7b-warm](#toc0_)\n",
    "- Description: NorMistral-7b-warm is a large Norwegian language model initialized from Mistral-7b-v0.1 and continuously pretrained on a total of 260 billion subword tokens (using six repetitions of open Norwegian texts). [Read more here...](https://huggingface.co/norallm/normistral-7b-warm)\n",
    "- Model Developers: Language Technology Group at the University of Oslo.\n",
    "- Model Architecture: NorMistral is an auto-regressive language model that uses an optimized transformer architecture based on the Mistral/Llama language models.\n",
    "- License: [apache-2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md)\n",
    "- Code Repository: [GitHub](norallm/NorMistral-7b-warm.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [NorMistral-7b-warm](#toc1_)    \n",
    "  - [Install libraries](#toc1_1_)    \n",
    "  - [Load libraries](#toc1_2_)    \n",
    "  - [Load tokenizer and model](#toc1_3_)    \n",
    "  - [Prompt the model](#toc1_4_)    \n",
    "  - [Generate text](#toc1_5_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Install libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers bitsandbytes accelerate datasets torch --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Load libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Load tokenizer and model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"norallm/normistral-7b-warm\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"norallm/normistral-7b-warm\").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Prompt the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt = \"\"\"Engelsk: {0}\n",
    "Bokm√•l:\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(text):\n",
    "    text = prompt.format(text)\n",
    "    input_ids = tokenizer(text, return_tensors='pt').input_ids.cuda()\n",
    "    prediction = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=False,\n",
    "        eos_token_id=tokenizer('\\n').input_ids\n",
    "    )\n",
    "    return tokenizer.decode(prediction[0, input_ids.size(1):]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Generate text](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(\"I'm super excited about this Norwegian NORA model! Can it translate these sentences?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
