# NORA.LLM

## Introduction

The collection of Large Language Models for the Norwegian language, [NorLM](http://norlm.nlpl.eu/) (now more commonly known as NORA.LLM), originated as a collaborative initiative involving several notable projects. These include [EOSC-Nordic](https://www.eosc-nordic.eu/) (European Open Science Cloud), [SANT](https://www.mn.uio.no/ifi/english/research/projects/sant/index.html) (Sentiment Analysis for Norwegian), and [HPLT](https://hplt-project.org/) (High-Performance Language Technologies). The initiative was developed in partnership with the [AI Laboratory of the National Library of Norway](https://ai.nb.no/) and the [National e-Infrastructure Services](https://www.sigma2.no/), under the coordination of the [Language Technology Group](https://www.mn.uio.no/ifi/english/research/groups/ltg/) (LTG) at the University of Oslo.

The team behind NorLM aims to provide these models and complementary tools for researchers and developers in the field of Natural Language Processing (NLP) focused on the Norwegian language. Their objective is to facilitate scientific experimentation and enable practical applications using cutting-edge NLP architectures. Moreover, they aspire to empower others to build their own large-scale models for various tasks, whether domain- or application-specific, or even for different language variants or other languages altogether.

## Models

The NorLM initiative has produced several notable models tailored for various NLP applications in the Norwegian language. These models include:

### NorMistral-7b-warm

- Description: NorMistral-7b-warm is a large Norwegian language model initialized from Mistral-7b-v0.1 and continuously pretrained on a total of 260 billion subword tokens (using six repetitions of open Norwegian texts). [Read more here...](https://huggingface.co/norallm/normistral-7b-warm)
- Model Developers: Language Technology Group at the University of Oslo.
- Model Architecture: NorMistral is an auto-regressive language model that uses an optimized transformer architecture based on the Mistral/Llama language models.
- License: [apache-2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md)
- Code Repository: [GitHub](https://github.com/NORA-Norwegian-AI-Research-Consortium/ai-fjord/blob/b15def2d105636f579a7eb1b1f5f1d6fdac4c891/norallm/NorMistral-7b-warm.ipynb)

### NorMistral-7b-warm-instruct

- Description: NorMistral-7b-warm-instruct is a large Norwegian language model initialized from Mistral-7b-v0.1 and continuously pretrained on a total of 260 billion subword tokens (using six repetitions of open Norwegian texts). [Read more here...](https://huggingface.co/norallm/normistral-7b-warm-instruct)
- Model Developers: Language Technology Group at the University of Oslo.
- Model Architecture: NorMistral is an auto-regressive language model that uses an optimized transformer architecture based on the Mistral/Llama language models.
- License: [apache-2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md)
- Code Repository: [GitHub](https://github.com/NORA-Norwegian-AI-Research-Consortium/ai-fjord/blob/b15def2d105636f579a7eb1b1f5f1d6fdac4c891/norallm/NorMistral-7b-warm-instruct.ipynb)

### NorMistral-7b-scratch

- Description: NorMistral-7b-scratch is a large Norwegian language model pretrained from scratch on a total of 260 billion subword tokens (using six repetitions of open Norwegian texts). [Read more here...](https://huggingface.co/norallm/normistral-7b-scratch)
- Model Developers: Language Technology Group at the University of Oslo.
- Model Architecture: NorMistral is an auto-regressive language model that uses an optimized transformer architecture based on the Mistral/Llama language models.
- License: [apache-2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md)
- Code Repository: [GitHub](https://github.com/NORA-Norwegian-AI-Research-Consortium/ai-fjord/blob/b15def2d105636f579a7eb1b1f5f1d6fdac4c891/norallm/NorMistral-7b-scratch.ipynb)

### NorBLOOM-7b-scratch

- Description: NorBLOOM-7b-scratch is a large Norwegian language model pretrained from scratch on a total of 260 billion subword tokens (using six repetitions of open Norwegian texts). [Read more here...](https://huggingface.co/norallm/norbloom-7b-scratch)
- Model Developers: Language Technology Group at the University of Oslo.
- Model Architecture: NorBLOOM is an auto-regressive language model that uses the BLOOM architecture.
- License: [apache-2.0](https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.md)
- Code Repository: [GitHub](https://github.com/NORA-Norwegian-AI-Research-Consortium/ai-fjord/blob/b15def2d105636f579a7eb1b1f5f1d6fdac4c891/norallm/NorBLOOM-7b-scratch.ipynb)

## Expert Group

## NAIC Access
